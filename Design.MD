## Design
If I were going to continue improving and optimizing my ETL process, I would move away from using `csv.DictReader` to read the input file. While it seened useful early in the process to be able to access values in a row by field name, my sense is that mapping each row to a dictionary is very time consuming and can lead to an unnecessarily high error rate as one missing or incorrect character can cause values to be associated with the wrong field name. I would instead move to reading each row as a string and refining my grok patterns and regular expressions to extract the relevant values, instead of just as an extra validation step. While I am not very familiar with the time complexity of grok and regex, I think that at least removing the step of mapping the string to a dictionary would streamline the process. I imagine pattern matching to be time intensive itself, so I would continue looking for ways to optimize those expressions once I was extracting all of the values in that way.

I would also ask about how to handle rows with incomplete data or `0` and `[]` values for relevant fields. Should those rows be included in the results? This leads me to how I would continue to build and optimize error handling. Currently, if the value for any field in a row is returned as `None` from the grok or regex validation, that row is written to an error log and left out of the data model. Should rows with values of `0` for budget or revenue, or lists of length `0` for production companies and genres cause rows to be left out of the data model? Or should all incomplete data be included in the data model anyway? I think my next step would be to examine the rows being written to the error log and identify ways to improve my pattern matching expressions, or perhaps if a second level of pattern matching could be used to extract data from rows that fail at the first level, and thereby bring down the rate of rows being written to the error log and increase the amount of data going into the data model.

Currently the execution time for this batch of data is 10-12 minutes on my personal machine, so with an increase of 100x data volume, hourly updates would not be feasible. My ETL script could be broken down into several functions, for example one dealing with movie data, another genres, and another production companies, and run on different machines. That would still leave the problem of each of those functions needing to read every line of data. Perhaps additional functionality could be added to the beginning of the pipline to break each row into chunks to be passed to the subsequent functions, so that each full row is only read once, and then subsequently smaller pieces are read by each subsequent function.  Large batches of data could also be systematically broken up into smaller batches to be distributed among many machines running the same ELT scripts, up to the point that the number of queries per second the database can handle became a limiting factor. Any of these ideas could require additional layers of overhead to coordinate the flow of data and scheduling of tasks, which would have their own considerations for space and time.

I imagine that when it comes to reprocessing or backfilling data, having those additional layers to coordinate the flow of data and scheduling of tasks would be important for moving historical data through the pipeline when system resources are available, so as not to impact the production flow. As my database grew, I would consider partitioning the data in order to make sorting and looking up data more efficient. In this case, movie release year would seem to be an obvious choice for a paritioning criteria, as the questions posed in this challenge all involved sorting data by year. This would also be useful in the backfilling of historical data, as the historical data could be partitioned based on release year as a first sorting step.

While I am familiar with some of the principles and considerations for managing large volumes of data, up until now I have worked with relatively small data sets that have not required scaling beyond the capabilities of my personal machine. I am very excited about the prospect of working with data pipelines and data models at scale, and joining a strong team where I will have the opportunity to learn from senior engineers with a lot of knowledge and first-hand experience tackling these types of questions.